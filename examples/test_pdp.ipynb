{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-13T07:25:40.426636Z",
     "start_time": "2024-08-13T07:25:40.422858Z"
    }
   },
   "source": [
    "import torch\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, RichModelSummary\n",
    "\n",
    "from rl4co.envs import PDPEnv\n",
    "from rl4co.models.zoo import AttentionModel\n",
    "from rl4co.utils.trainer import RL4COTrainer"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T07:38:49.803143Z",
     "start_time": "2024-08-13T07:38:49.795500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Callable, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "from tensordict.tensordict import TensorDict\n",
    "from torch.distributions import Uniform\n",
    "\n",
    "from rl4co.envs.common.utils import Generator, get_sampler\n",
    "from rl4co.utils.pylogger import get_pylogger\n",
    "\n",
    "log = get_pylogger(__name__)\n",
    "\n",
    "\n",
    "class PDPGenerator(Generator):\n",
    "    \"\"\"Data generator for the Pickup and Delivery Problem (PDP).\n",
    "    Args:\n",
    "        num_loc: number of locations (customers) in the PDP, without the depot. (e.g. 10 means 10 locs + 1 depot)\n",
    "            - 1 depot\n",
    "            - `num_loc` / 2 pickup locations\n",
    "            - `num_loc` / 2 delivery locations\n",
    "        min_loc: minimum value for the location coordinates\n",
    "        max_loc: maximum value for the location coordinates\n",
    "        init_sol_type: the method type used for generating initial solutions (random or greedy)\n",
    "        loc_distribution: distribution for the location coordinates\n",
    "        depot_distribution: distribution for the depot location. If None, sample the depot from the locations\n",
    "\n",
    "    Returns:\n",
    "        A TensorDict with the following keys:\n",
    "            locs [batch_size, num_loc, 2]: locations of each customer\n",
    "            depot [batch_size, 2]: location of the depot\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_loc: int = 20,\n",
    "        min_loc: float = 0.0,\n",
    "        max_loc: float = 1.0,\n",
    "        init_sol_type: str = \"random\",\n",
    "        loc_distribution: Union[int, float, str, type, Callable] = Uniform,\n",
    "        depot_distribution: Union[int, float, str, type, Callable] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.num_loc = num_loc\n",
    "        self.min_loc = min_loc\n",
    "        self.max_loc = max_loc\n",
    "        self.init_sol_type = init_sol_type\n",
    "\n",
    "        # Number of locations must be even\n",
    "        if num_loc % 2 != 0:\n",
    "            log.warn(\n",
    "                \"Number of locations must be even. Adding 1 to the number of locations.\"\n",
    "            )\n",
    "            self.num_loc += 1\n",
    "\n",
    "        # Location distribution\n",
    "        if kwargs.get(\"loc_sampler\", None) is not None:\n",
    "            self.loc_sampler = kwargs[\"loc_sampler\"]\n",
    "        else:\n",
    "            self.loc_sampler = get_sampler(\n",
    "                \"loc\", loc_distribution, min_loc, max_loc, **kwargs\n",
    "            )\n",
    "\n",
    "        # Depot distribution\n",
    "        if kwargs.get(\"depot_sampler\", None) is not None:\n",
    "            self.depot_sampler = kwargs[\"depot_sampler\"]\n",
    "        else:\n",
    "            self.depot_sampler = get_sampler(\n",
    "                \"depot\", depot_distribution, min_loc, max_loc, **kwargs\n",
    "            ) if depot_distribution is not None else None\n",
    "\n",
    "    def _generate(self, batch_size) -> TensorDict:\n",
    "        # Sample locations: depot and customers\n",
    "        if self.depot_sampler is not None:\n",
    "            depot = self.depot_sampler.sample((*batch_size, 2))\n",
    "            locs = self.loc_sampler.sample((*batch_size, self.num_loc, 2)) \n",
    "            charging = self.depot_sampler.sample((*batch_size, 2))\n",
    "        else:\n",
    "            # if depot_sampler is None, sample the depot from the locations\n",
    "            locs = self.loc_sampler.sample((*batch_size, self.num_loc + 2, 2))\n",
    "            depot = locs[..., 0, :]\n",
    "            charging = locs[..., 1, :]\n",
    "            locs = locs[..., 2:, :]\n",
    "\n",
    "        return TensorDict(\n",
    "            {\n",
    "                \"locs\": locs,\n",
    "                \"depot\": depot,\n",
    "                \"charging\": charging\n",
    "            },\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n"
   ],
   "id": "3f562b11c5970bf7",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T12:43:17.724923Z",
     "start_time": "2024-08-13T12:43:17.702989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rl4co.envs.routing.pdp.render import render\n",
    "# from rl4co.envs.routing.pdp.generator import PDPGenerator\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from tensordict.tensordict import TensorDict\n",
    "from torchrl.data import (\n",
    "    BoundedTensorSpec,\n",
    "    CompositeSpec,\n",
    "    UnboundedContinuousTensorSpec,\n",
    "    UnboundedDiscreteTensorSpec,\n",
    ")\n",
    "\n",
    "from rl4co.envs.common.base import ImprovementEnvBase, RL4COEnvBase\n",
    "from rl4co.utils.ops import gather_by_index, get_tour_length, get_distance\n",
    "\n",
    "\n",
    "class PDPEnv(RL4COEnvBase):\n",
    "    \"\"\"Pickup and Delivery Problem (PDP) environment.\n",
    "    The environment is made of num_loc + 1 locations (cities):\n",
    "        - 1 depot\n",
    "        - `num_loc` / 2 pickup locations\n",
    "        - `num_loc` / 2 delivery locations\n",
    "    The goal is to visit all the pickup and delivery locations in the shortest path possible starting from the depot\n",
    "    The conditions is that the agent must visit a pickup location before visiting its corresponding delivery location\n",
    "\n",
    "    Observations:\n",
    "        - locations of the depot, pickup, and delivery locations\n",
    "        - current location of the vehicle\n",
    "        - the remaining locations to deliver\n",
    "        - the visited locations\n",
    "        - the current step\n",
    "\n",
    "    Constraints:\n",
    "        - the tour starts and ends at the depot\n",
    "        - each pickup location must be visited before its corresponding delivery location\n",
    "        - the vehicle cannot visit the same location twice\n",
    "\n",
    "    Finish Condition:\n",
    "        - the vehicle has visited all locations\n",
    "\n",
    "    Reward:\n",
    "        - (minus) the negative length of the path\n",
    "\n",
    "    Args:\n",
    "        generator: PDPGenerator instance as the data generator\n",
    "        generator_params: parameters for the generator\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"pdp\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator: PDPGenerator = None,\n",
    "        generator_params: dict = {},\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        if generator is None:\n",
    "            generator = PDPGenerator(**generator_params)\n",
    "        self.generator = generator\n",
    "        self._make_spec(self.generator)\n",
    "\n",
    "    def _step(self, td: TensorDict) -> TensorDict:\n",
    "        print(\"action\", td[\"action\"])\n",
    "        print(\"current_node\", td[\"current_node\"], td[\"current_node\"].shape)\n",
    "        dist_mat = self._get_distance_matrix(td[\"locs\"])\n",
    "        cur_node_to_all = gather_by_index(dist_mat, idx=td[\"current_node\"])\n",
    "        print(cur_node_to_all.shape)\n",
    "        print(\"dist_mat_distance\", gather_by_index(cur_node_to_all, idx=td[\"action\"]))\n",
    "        start_node = gather_by_index(td[\"locs\"], idx=td[\"current_node\"].unsqueeze(-1))\n",
    "        end_node = gather_by_index(td[\"locs\"], idx=td[\"action\"])\n",
    "        dist_traveled = get_distance(start_node, end_node)\n",
    "        current_node = td[\"action\"].unsqueeze(-1)\n",
    "        print(\"other dist\", dist_traveled)\n",
    "        batch_size = td[\"locs\"].shape[0]\n",
    "\n",
    "        td[\"battery\"] -= 10 * dist_traveled.view(td[\"battery\"].shape)\n",
    "        battery_check = td[\"battery\"] < td[\"threshold\"] \n",
    "        # if battery_check.any():\n",
    "        #     print(\"battery low\")\n",
    "        #     print(td[\"charging_mask\"])\n",
    "        filtered_matrices = []\n",
    "        for i in range(dist_mat.size(0)):\n",
    "            matrix = dist_mat[i]\n",
    "            action_index = actions[i]\n",
    "            \n",
    "            # Select rows excluding the action_index row\n",
    "            filtered_matrix = matrix[action_index, :]\n",
    "            \n",
    "            # Append to the list\n",
    "            filtered_matrices.append(filtered_matrix)\n",
    "        \n",
    "        # Stack the list of filtered matrices into a single tensor\n",
    "        curr_node_to_all_dist = torch.stack(filtered_matrices)\n",
    "        #         curr_node_to_all_dist[:,1] = 0 \n",
    "\n",
    "        # find what locations can be reached with current state of charge\n",
    "        exceeds_charge = td[\"battery\"] - curr_node_to_all_dist > td[\"threshold\"]\n",
    "        num_loc = td[\"locs\"].shape[-2] - 2  # except depot and charging station\n",
    "        # Pickup and delivery node pair of selected node\n",
    "        new_to_deliver = (current_node + num_loc // 2) % (num_loc + 1)\n",
    "        # Set available to 0 (i.e., we visited the node)\n",
    "        available = td[\"available\"].scatter(\n",
    "            -1, current_node.expand_as(td[\"action_mask\"]), 0\n",
    "        )\n",
    "\n",
    "        to_deliver = td[\"to_deliver\"].scatter(\n",
    "            -1, new_to_deliver.expand_as(td[\"to_deliver\"]), 1\n",
    "        )\n",
    "\n",
    "        # Action is feasible if the node is not visited and is to deliver\n",
    "        # action_mask = torch.logical_and(available, to_deliver)       \n",
    "        action_mask = available & to_deliver \n",
    "\n",
    "        # We are done there are no unvisited locations\n",
    "        done = torch.count_nonzero(available, dim=-1) == 0\n",
    "        print(action_mask)\n",
    "        # The reward is calculated outside via get_reward for efficiency, so we set it to 0 here\n",
    "        reward = torch.zeros_like(done)\n",
    "\n",
    "        # Update step\n",
    "        td.update(\n",
    "            {\n",
    "                \"current_node\": current_node,\n",
    "                \"available\": available,\n",
    "                \"to_deliver\": to_deliver,\n",
    "                \"i\": td[\"i\"] + 1,\n",
    "                \"action_mask\": action_mask,\n",
    "                \"reward\": reward,\n",
    "                \"done\": done,\n",
    "            }\n",
    "        )\n",
    "        return td\n",
    "\n",
    "    def _reset(self, td: Optional[TensorDict] = None, batch_size=None) -> TensorDict:\n",
    "        device = td.device\n",
    "\n",
    "        locs = torch.cat((td[\"depot\"][:, None, :], td[\"charging\"][:, None, :], td[\"locs\"], ), -2)\n",
    "        print(self.generator.num_loc)\n",
    "        # Pick is 1, deliver is 0 [batch_size, graph_size+1], [1,1...1, 0...0]\n",
    "        to_deliver = torch.cat(\n",
    "            [\n",
    "                torch.ones(\n",
    "                    *batch_size,\n",
    "                    self.generator.num_loc // 2 + 2,\n",
    "                    dtype=torch.bool,\n",
    "                ).to(device),\n",
    "                torch.zeros(\n",
    "                    *batch_size,\n",
    "                    self.generator.num_loc // 2 ,\n",
    "                    dtype=torch.bool,\n",
    "                ).to(device),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # num_loc = locs.shape[-2] - 2 \n",
    "        # charging_station = torch.ones((*batch_size, 1), dtype=torch.int64).to(device)\n",
    "        # new_to_deliver = (charging_station + num_loc // 2) % (num_loc + 1)\n",
    "        # to_deliver = to_deliver.scatter(\n",
    "        #     -1, new_to_deliver.expand_as(to_deliver), 0\n",
    "        # )\n",
    "\n",
    "        # Cannot visit depot at first step # [0,1...1] so set not available\n",
    "        available = torch.ones(\n",
    "            (*batch_size, self.generator.num_loc + 2), dtype=torch.bool\n",
    "        ).to(device)\n",
    "        action_mask = ~available.contiguous()  # [batch_size, graph_size+1]\n",
    "        action_mask[..., 0] = 1  # First step is always the depot\n",
    "        \n",
    "        charging_mask = ~available.contiguous()\n",
    "        charging_mask[..., 1] = 1\n",
    "        available[..., 1] = 0\n",
    "\n",
    "        # Other variables\n",
    "        current_node = torch.zeros((*batch_size, 1), dtype=torch.int64).to(device)\n",
    "        i = torch.zeros((*batch_size, 1), dtype=torch.int64).to(device)\n",
    "        # battery = torch.ones(\n",
    "        #     (*batch_size, self.generator.num_loc+2), dtype=torch.float32\n",
    "        # ).to(device)\n",
    "        battery = torch.full((*batch_size, 1), 100, dtype=torch.float32)\n",
    "        threshold = torch.full(\n",
    "                    (*batch_size, 1), 30, device=device\n",
    "                )        \n",
    "        return TensorDict(\n",
    "            {\n",
    "                \"locs\": locs,\n",
    "                \"current_node\": current_node,\n",
    "                \"to_deliver\": to_deliver,\n",
    "                \"available\": available,\n",
    "                \"i\": i,\n",
    "                \"action_mask\": action_mask,\n",
    "                \"charging_mask\": charging_mask,\n",
    "                \"battery\": battery,\n",
    "                \"threshold\": threshold\n",
    "            },\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_distance_matrix(locs: torch.Tensor):\n",
    "        \"\"\"Compute the Manhattan distance matrix for the given coordinates.\n",
    "\n",
    "        Args:\n",
    "            locs: Tensor of shape [..., n, dim]\n",
    "        \"\"\"\n",
    "        if locs.dtype != torch.float32 and locs.dtype != torch.float64:\n",
    "            locs = locs.to(torch.float32)\n",
    "\n",
    "            # Compute pairwise differences\n",
    "        diff = locs[..., :, None, :] - locs[..., None, :, :]\n",
    "\n",
    "        # Compute Manhattan distance\n",
    "        distance_matrix = torch.sum(torch.abs(diff), dim=-1)\n",
    "        return distance_matrix\n",
    "    \n",
    "    def _make_spec(self, generator: PDPGenerator):\n",
    "        \"\"\"Make the observation and action specs from the parameters.\"\"\"\n",
    "        self.observation_spec = CompositeSpec(\n",
    "            locs=BoundedTensorSpec(\n",
    "                low=generator.min_loc,\n",
    "                high=generator.max_loc,\n",
    "                shape=(generator.num_loc + 1, 2),\n",
    "                dtype=torch.float32,\n",
    "            ),\n",
    "            current_node=UnboundedDiscreteTensorSpec(\n",
    "                shape=(1),\n",
    "                dtype=torch.int64,\n",
    "            ),\n",
    "            to_deliver=UnboundedDiscreteTensorSpec(\n",
    "                shape=(1),\n",
    "                dtype=torch.int64,\n",
    "            ),\n",
    "            i=UnboundedDiscreteTensorSpec(\n",
    "                shape=(1),\n",
    "                dtype=torch.int64,\n",
    "            ),\n",
    "            action_mask=UnboundedDiscreteTensorSpec(\n",
    "                shape=(generator.num_loc + 1),\n",
    "                dtype=torch.bool,\n",
    "            ),\n",
    "            shape=(),\n",
    "        )\n",
    "        self.action_spec = BoundedTensorSpec(\n",
    "            shape=(1,),\n",
    "            dtype=torch.int64,\n",
    "            low=0,\n",
    "            high=generator.num_loc + 1,\n",
    "        )\n",
    "        self.reward_spec = UnboundedContinuousTensorSpec(shape=(1,))\n",
    "        self.done_spec = UnboundedDiscreteTensorSpec(shape=(1,), dtype=torch.bool)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_reward(td, actions) -> TensorDict:\n",
    "        # Gather locations in order of tour (add depot since we start and end there)\n",
    "        locs_ordered = torch.cat(\n",
    "            [\n",
    "                td[\"locs\"][..., 0:1, :],  # depot\n",
    "                gather_by_index(td[\"locs\"], actions),  # order locations\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        return -get_tour_length(locs_ordered)\n",
    "\n",
    "    def check_solution_validity(self, td, actions):\n",
    "        # assert (actions[:, 0] == 0).all(), \"Not starting at depot\"\n",
    "        assert (\n",
    "            torch.arange(actions.size(1), out=actions.data.new())\n",
    "            .view(1, -1)\n",
    "            .expand_as(actions)\n",
    "            == actions.data.sort(1)[0]\n",
    "        ).all(), \"Not visiting all nodes\"\n",
    "\n",
    "        visited_time = torch.argsort(\n",
    "            actions, 1\n",
    "        )  # index of pickup less than index of delivery\n",
    "        assert (\n",
    "            visited_time[:, 1 : actions.size(1) // 2 + 1]\n",
    "            < visited_time[:, actions.size(1) // 2 + 1 :]\n",
    "        ).all(), \"Deliverying without pick-up\"\n",
    "\n",
    "    def get_num_starts(self, td):\n",
    "        \"\"\"Only half of the nodes (i.e. pickup nodes) can be start nodes\"\"\"\n",
    "        return (td[\"locs\"].shape[-2] - 1) // 2\n",
    "\n",
    "    def select_start_nodes(self, td, num_starts):\n",
    "        \"\"\"Only nodes from [1 : num_loc // 2 +1] (i.e. pickups) can be selected\"\"\"\n",
    "        num_possible_starts = (td[\"locs\"].shape[-2] - 1) // 2\n",
    "        selected = (\n",
    "            torch.arange(num_starts, device=td.device).repeat_interleave(td.shape[0])\n",
    "            % num_possible_starts\n",
    "            + 1\n",
    "        )\n",
    "        return selected\n",
    "\n",
    "    @staticmethod\n",
    "    def render(td: TensorDict, actions: torch.Tensor = None, ax=None):\n",
    "        return render(td, actions, ax)\n"
   ],
   "id": "c179c28b6a14664a",
   "outputs": [],
   "execution_count": 760
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T12:43:17.852121Z",
     "start_time": "2024-08-13T12:43:17.844740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class PDPInitEmbedding(nn.Module):\n",
    "    \"\"\"Initial embedding for the Pickup and Delivery Problem (PDP).\n",
    "    Embed the following node features to the embedding space:\n",
    "        - locs: x, y coordinates of the nodes (depot, pickups and deliveries separately)\n",
    "           Note that pickups and deliveries are interleaved in the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, linear_bias=True):\n",
    "        super(PDPInitEmbedding, self).__init__()\n",
    "        node_dim = 2  # x, y\n",
    "        self.init_embed_depot = nn.Linear(2, embed_dim, linear_bias)\n",
    "        self.init_embed_charging = nn.Linear(2, embed_dim, linear_bias)\n",
    "        self.init_embed_pick = nn.Linear(node_dim * 2, embed_dim, linear_bias)\n",
    "        self.init_embed_delivery = nn.Linear(node_dim, embed_dim, linear_bias)\n",
    "\n",
    "    def forward(self, td):\n",
    "        depot, charging, locs = td[\"locs\"][..., 0:1, :], td[\"locs\"][..., 1:2, :], td[\"locs\"][..., 2:, :]\n",
    "        num_locs = locs.size(-2)\n",
    "        pick_feats = torch.cat(\n",
    "            [locs[:, : num_locs // 2, :], locs[:, num_locs // 2 :, :]], -1\n",
    "        )  # [batch_size, graph_size//2, 4]\n",
    "        delivery_feats = locs[:, num_locs // 2 :, :]  # [batch_size, graph_size//2, 2]\n",
    "        depot_embeddings = self.init_embed_depot(depot)\n",
    "        charging_embeddings = self.init_embed_charging(charging)\n",
    "        pick_embeddings = self.init_embed_pick(pick_feats)\n",
    "        delivery_embeddings = self.init_embed_delivery(delivery_feats)\n",
    "        # concatenate on graph size dimension\n",
    "        return torch.cat([depot_embeddings, charging_embeddings, pick_embeddings, delivery_embeddings], -2)"
   ],
   "id": "e8cbea386083d9e0",
   "outputs": [],
   "execution_count": 761
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T12:43:18.036443Z",
     "start_time": "2024-08-13T12:43:18.031178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rl4co.models.nn.env_embeddings.context import EnvContext\n",
    "\n",
    "\n",
    "class PDPContext(EnvContext):\n",
    "    \"\"\"Context embedding for the Pickup and Delivery Problem (PDP).\n",
    "    Project the following to the embedding space:\n",
    "        - current node embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim):\n",
    "        super(PDPContext, self).__init__(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, embeddings, td):\n",
    "        cur_node_embedding = self._cur_node_embedding(embeddings, td).squeeze()\n",
    "        return self.project_context(cur_node_embedding)"
   ],
   "id": "d12ca6038e50a843",
   "outputs": [],
   "execution_count": 762
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T12:43:18.204221Z",
     "start_time": "2024-08-13T12:43:18.195877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RL4CO env based on TorchRL\n",
    "env = PDPEnv(generator_params=dict(num_loc=20))"
   ],
   "id": "e7c3170081ce22da",
   "outputs": [],
   "execution_count": 763
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T12:43:18.704035Z",
     "start_time": "2024-08-13T12:43:18.693483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from examples.slap import StaticEmbedding\n",
    "from rl4co.models.nn.env_embeddings.context import PDPContext\n",
    "from rl4co.models import AttentionModel, AttentionModelPolicy\n",
    "\n",
    "emb_dim = 128\n",
    "policy = AttentionModelPolicy(env_name=env.name, # this is actually not needed since we are initializing the embeddings!\n",
    "                              embed_dim=emb_dim,\n",
    "                              init_embedding=PDPInitEmbedding(emb_dim),\n",
    "                              context_embedding=PDPContext(emb_dim)\n",
    ")"
   ],
   "id": "6abe9de447d8e50f",
   "outputs": [],
   "execution_count": 764
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T12:43:19.120858Z",
     "start_time": "2024-08-13T12:43:19.092580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model: default is AM with REINFORCE and greedy rollout baseline\n",
    "model = AttentionModel(env,\n",
    "                       baseline='rollout',\n",
    "                       policy=policy,\n",
    "                       train_data_size=100_000, # really small size for demo\n",
    "                       val_data_size=10_000)"
   ],
   "id": "bb7a43326453bcaa",
   "outputs": [],
   "execution_count": 765
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T12:43:19.632695Z",
     "start_time": "2024-08-13T12:43:19.478021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Greedy rollouts over untrained policy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "td_init = env.reset(batch_size=[4]).to(device)\n",
    "policy = model.policy.to(device)\n",
    "out = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\n",
    "\n",
    "# Plotting\n",
    "print(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\")\n",
    "for td, actions in zip(td_init, out['actions'].cpu()):\n",
    "    env.render(td, actions)"
   ],
   "id": "401279f4c1e5879c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "action tensor([0, 0, 0, 0])\n",
      "current_node tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0., 0., 0., 0.])\n",
      "other dist tensor([0., 0., 0., 0.])\n",
      "tensor([[False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False]])\n",
      "action tensor([11,  9,  9,  9])\n",
      "current_node tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.6872, 0.6091, 1.0286, 0.5656])\n",
      "other dist tensor([0.6143, 0.4308, 0.8086, 0.5089])\n",
      "tensor([[False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "          True,  True, False, False, False, False, False, False, False,  True,\n",
      "         False, False],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "          True,  True, False, False, False, False, False, False, False,  True,\n",
      "         False, False],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "          True,  True, False, False, False, False, False, False, False,  True,\n",
      "         False, False]])\n",
      "action tensor([ 6, 19, 19, 19])\n",
      "current_node tensor([[11],\n",
      "        [ 9],\n",
      "        [ 9],\n",
      "        [ 9]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.2783, 0.6357, 0.9375, 0.9379])\n",
      "other dist tensor([0.1983, 0.5307, 0.8270, 0.8392])\n",
      "tensor([[False, False,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "          True, False, False, False, False, False,  True, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "          True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "          True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "          True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False]])\n",
      "action tensor([16,  4, 11, 11])\n",
      "current_node tensor([[ 6],\n",
      "        [19],\n",
      "        [19],\n",
      "        [19]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.5801, 0.8741, 0.2904, 0.6549])\n",
      "other dist tensor([0.5235, 0.6402, 0.2483, 0.4665])\n",
      "tensor([[False, False,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True, False,  True,  True,  True,  True, False,\n",
      "          True,  True, False, False,  True, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False]])\n",
      "action tensor([10, 14,  2,  5])\n",
      "current_node tensor([[16],\n",
      "        [ 4],\n",
      "        [11],\n",
      "        [11]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.8168, 0.6549, 0.2569, 0.6482])\n",
      "other dist tensor([0.6851, 0.4793, 0.1878, 0.4814])\n",
      "tensor([[False, False,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "          True, False],\n",
      "        [False, False,  True,  True, False,  True,  True,  True,  True, False,\n",
      "          True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False, False,  True,  True,  True,  True,  True,  True, False,\n",
      "          True, False,  True, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True,  True, False,  True,  True,  True, False,\n",
      "          True, False, False, False, False,  True, False, False, False, False,\n",
      "         False, False]])\n",
      "action tensor([20, 10, 12, 15])\n",
      "current_node tensor([[10],\n",
      "        [14],\n",
      "        [ 2],\n",
      "        [ 5]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.4320, 0.7988, 0.7026, 0.2639])\n",
      "other dist tensor([0.3924, 0.6347, 0.6775, 0.2352])\n",
      "tensor([[False, False,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         False,  True, False, False, False, False, False, False, False, False,\n",
      "          True, False],\n",
      "        [False, False, False,  True,  True,  True,  True,  True,  True, False,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True,  True, False,  True,  True,  True, False,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False]])\n",
      "action tensor([4, 8, 3, 4])\n",
      "current_node tensor([[20],\n",
      "        [10],\n",
      "        [12],\n",
      "        [15]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.4981, 1.1189, 0.9052, 0.5252])\n",
      "other dist tensor([0.4079, 0.9724, 0.6472, 0.4662])\n",
      "tensor([[False, False,  True,  True, False,  True, False,  True,  True,  True,\n",
      "         False, False, False, False,  True, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True, False,  True,  True,  True, False, False,\n",
      "         False,  True, False, False, False, False, False, False,  True, False,\n",
      "          True, False],\n",
      "        [False, False, False, False,  True,  True,  True,  True,  True, False,\n",
      "          True, False, False,  True, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True, False, False,  True,  True,  True, False,\n",
      "          True, False, False, False,  True, False, False, False, False, False,\n",
      "         False, False]])\n",
      "action tensor([ 5, 18,  8, 14])\n",
      "current_node tensor([[4],\n",
      "        [8],\n",
      "        [3],\n",
      "        [4]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([1.5295, 0.5114, 0.1338, 0.3596])\n",
      "other dist tensor([1.0926, 0.4355, 0.1324, 0.3382])\n",
      "tensor([[False, False,  True,  True, False, False, False,  True,  True,  True,\n",
      "         False, False, False, False,  True,  True, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True, False,  True,  True,  True, False, False,\n",
      "         False,  True, False, False, False, False, False, False, False, False,\n",
      "          True, False],\n",
      "        [False, False, False, False,  True,  True,  True,  True, False, False,\n",
      "          True, False, False,  True, False, False, False, False,  True, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True, False, False,  True,  True,  True, False,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False]])\n",
      "action tensor([ 8,  5, 10,  3])\n",
      "current_node tensor([[ 5],\n",
      "        [18],\n",
      "        [ 8],\n",
      "        [14]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.9576, 0.0269, 0.8174, 0.3616])\n",
      "other dist tensor([0.8724, 0.0200, 0.5829, 0.2593])\n",
      "tensor([[False, False,  True,  True, False, False, False,  True, False,  True,\n",
      "         False, False, False, False,  True,  True, False, False,  True, False,\n",
      "         False, False],\n",
      "        [False, False,  True,  True, False, False,  True,  True, False, False,\n",
      "         False,  True, False, False, False,  True, False, False, False, False,\n",
      "          True, False],\n",
      "        [False, False, False, False,  True,  True,  True,  True, False, False,\n",
      "         False, False, False,  True, False, False, False, False,  True, False,\n",
      "          True, False],\n",
      "        [False, False,  True, False, False, False,  True,  True,  True, False,\n",
      "          True, False, False,  True, False, False, False, False, False, False,\n",
      "         False, False]])\n",
      "action tensor([ 9,  7,  4, 13])\n",
      "current_node tensor([[ 8],\n",
      "        [ 5],\n",
      "        [10],\n",
      "        [ 3]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.2410, 0.1906, 0.1928, 0.4671])\n",
      "other dist tensor([0.2223, 0.1593, 0.1918, 0.3407])\n",
      "tensor([[False, False,  True,  True, False, False, False,  True, False, False,\n",
      "         False, False, False, False,  True,  True, False, False,  True,  True,\n",
      "         False, False],\n",
      "        [False, False,  True,  True, False, False,  True, False, False, False,\n",
      "         False,  True, False, False, False,  True, False,  True, False, False,\n",
      "          True, False],\n",
      "        [False, False, False, False, False,  True,  True,  True, False, False,\n",
      "         False, False, False,  True,  True, False, False, False,  True, False,\n",
      "          True, False],\n",
      "        [False, False,  True, False, False, False,  True,  True,  True, False,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False]])\n",
      "action tensor([ 3, 11,  6,  7])\n",
      "current_node tensor([[ 9],\n",
      "        [ 7],\n",
      "        [ 4],\n",
      "        [13]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.4803, 0.8604, 0.3589, 0.4378])\n",
      "other dist tensor([0.3825, 0.6136, 0.2538, 0.3098])\n",
      "tensor([[False, False,  True, False, False, False, False,  True, False, False,\n",
      "         False, False, False,  True,  True,  True, False, False,  True,  True,\n",
      "         False, False],\n",
      "        [False, False,  True,  True, False, False,  True, False, False, False,\n",
      "         False, False, False, False, False,  True, False,  True, False, False,\n",
      "          True, False],\n",
      "        [False, False, False, False, False,  True, False,  True, False, False,\n",
      "         False, False, False,  True,  True, False,  True, False,  True, False,\n",
      "          True, False],\n",
      "        [False, False,  True, False, False, False,  True, False,  True, False,\n",
      "          True, False, False, False, False, False, False,  True, False, False,\n",
      "         False, False]])\n",
      "action tensor([ 2,  6,  7, 17])\n",
      "current_node tensor([[ 3],\n",
      "        [11],\n",
      "        [ 6],\n",
      "        [ 7]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.6095, 0.7500, 0.9074, 0.1350])\n",
      "other dist tensor([0.5035, 0.5309, 0.6494, 0.1050])\n",
      "tensor([[False, False, False, False, False, False, False,  True, False, False,\n",
      "         False, False,  True,  True,  True,  True, False, False,  True,  True,\n",
      "         False, False],\n",
      "        [False, False,  True,  True, False, False, False, False, False, False,\n",
      "         False, False, False, False, False,  True,  True,  True, False, False,\n",
      "          True, False],\n",
      "        [False, False, False, False, False,  True, False, False, False, False,\n",
      "         False, False, False,  True,  True, False,  True,  True,  True, False,\n",
      "          True, False],\n",
      "        [False, False,  True, False, False, False,  True, False,  True, False,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False]])\n",
      "action tensor([7, 2, 5, 6])\n",
      "current_node tensor([[ 2],\n",
      "        [ 6],\n",
      "        [ 7],\n",
      "        [17]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.8481, 0.8431, 0.2221, 0.8259])\n",
      "other dist tensor([0.6200, 0.5965, 0.1906, 0.6135])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "         False, False],\n",
      "        [False, False, False,  True, False, False, False, False, False, False,\n",
      "         False, False,  True, False, False,  True,  True,  True, False, False,\n",
      "          True, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False,  True,  True,  True,  True,  True,  True, False,\n",
      "          True, False],\n",
      "        [False, False,  True, False, False, False, False, False,  True, False,\n",
      "          True, False, False, False, False, False,  True, False, False, False,\n",
      "         False, False]])\n",
      "action tensor([17,  3, 15, 16])\n",
      "current_node tensor([[7],\n",
      "        [2],\n",
      "        [5],\n",
      "        [6]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.3422, 0.4463, 0.5636, 0.8081])\n",
      "other dist tensor([0.2509, 0.4329, 0.4027, 0.6174])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False,  True,  True,  True,  True, False, False,  True,  True,\n",
      "         False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False,  True,  True, False,  True,  True,  True, False, False,\n",
      "          True, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False,  True,  True, False,  True,  True,  True, False,\n",
      "          True, False],\n",
      "        [False, False,  True, False, False, False, False, False,  True, False,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False]])\n",
      "action tensor([19, 12, 17,  8])\n",
      "current_node tensor([[17],\n",
      "        [ 3],\n",
      "        [15],\n",
      "        [16]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.5774, 0.8641, 0.1043, 0.3707])\n",
      "other dist tensor([0.4085, 0.6121, 0.0737, 0.3682])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False,  True,  True,  True,  True, False, False,  True, False,\n",
      "         False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False,  True, False,  True,  True,  True, False, False,\n",
      "          True, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False,  True,  True, False,  True, False,  True, False,\n",
      "          True, False],\n",
      "        [False, False,  True, False, False, False, False, False, False, False,\n",
      "          True, False, False, False, False, False, False, False,  True, False,\n",
      "         False, False]])\n",
      "action tensor([12, 13, 13, 18])\n",
      "current_node tensor([[19],\n",
      "        [12],\n",
      "        [17],\n",
      "        [ 8]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.2683, 0.4615, 0.2580, 0.3865])\n",
      "other dist tensor([0.2205, 0.3429, 0.1844, 0.2988])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False,  True,  True,  True, False, False,  True, False,\n",
      "         False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False,  True,  True,  True, False, False,\n",
      "          True, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False,  True, False,  True, False,  True, False,\n",
      "          True, False],\n",
      "        [False, False,  True, False, False, False, False, False, False, False,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False]])\n",
      "action tensor([13, 15, 14, 10])\n",
      "current_node tensor([[12],\n",
      "        [13],\n",
      "        [13],\n",
      "        [18]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([1.0054, 0.4305, 0.1716, 0.3131])\n",
      "other dist tensor([0.7115, 0.3961, 0.1625, 0.2768])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False,  True,  True, False, False,  True, False,\n",
      "         False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True,  True, False, False,\n",
      "          True, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True, False,  True, False,\n",
      "          True, False],\n",
      "        [False, False,  True, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "          True, False]])\n",
      "action tensor([18, 16, 16,  2])\n",
      "current_node tensor([[13],\n",
      "        [15],\n",
      "        [14],\n",
      "        [10]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.5600, 0.4584, 0.1877, 0.3409])\n",
      "other dist tensor([0.4799, 0.3247, 0.1330, 0.2738])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False,  True,  True, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False,  True, False, False,\n",
      "          True, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False,  True, False,\n",
      "          True, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False,  True, False, False, False, False, False, False, False,\n",
      "          True, False]])\n",
      "action tensor([15, 20, 20, 12])\n",
      "current_node tensor([[18],\n",
      "        [16],\n",
      "        [16],\n",
      "        [ 2]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.5036, 0.4894, 0.8609, 0.8118])\n",
      "other dist tensor([0.3709, 0.3475, 0.7285, 0.5749])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False,  True, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False,  True, False, False,\n",
      "         False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False,  True, False,\n",
      "         False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "          True, False]])\n",
      "action tensor([14, 17, 18, 20])\n",
      "current_node tensor([[15],\n",
      "        [20],\n",
      "        [20],\n",
      "        [12]]) torch.Size([4, 1])\n",
      "torch.Size([4, 22])\n",
      "dist_mat_distance tensor([0.2078, 0.8652, 0.0582, 0.9076])\n",
      "other dist tensor([0.1545, 0.6554, 0.0437, 0.6418])\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Logits contain NaNs",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[766], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m td_init \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset(batch_size\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m4\u001B[39m])\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      4\u001B[0m policy \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 5\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtd_init\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mphase\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgreedy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_actions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Plotting\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTour lengths: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m-\u001B[39mr\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mr\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mout[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreward\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\rl4co\\models\\common\\constructive\\base.py:230\u001B[0m, in \u001B[0;36mConstructivePolicy.forward\u001B[1;34m(self, td, env, phase, calc_reward, return_actions, return_entropy, return_hidden, return_init_embeds, return_sum_log_likelihood, actions, max_steps, **decoding_kwargs)\u001B[0m\n\u001B[0;32m    228\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m td[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdone\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mall():\n\u001B[1;32m--> 230\u001B[0m     logits, mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_starts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m     td \u001B[38;5;241m=\u001B[39m decode_strategy\u001B[38;5;241m.\u001B[39mstep(\n\u001B[0;32m    232\u001B[0m         logits,\n\u001B[0;32m    233\u001B[0m         mask,\n\u001B[0;32m    234\u001B[0m         td,\n\u001B[0;32m    235\u001B[0m         action\u001B[38;5;241m=\u001B[39mactions[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, step] \u001B[38;5;28;01mif\u001B[39;00m actions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    236\u001B[0m     )\n\u001B[0;32m    237\u001B[0m     td \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(td)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\rl4co\\models\\zoo\\am\\decoder.py:191\u001B[0m, in \u001B[0;36mAttentionModelDecoder.forward\u001B[1;34m(self, td, cached, num_starts)\u001B[0m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;66;03m# Compute logits\u001B[39;00m\n\u001B[0;32m    190\u001B[0m mask \u001B[38;5;241m=\u001B[39m td[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maction_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 191\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpointer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mglimpse_q\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mglimpse_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mglimpse_v\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[38;5;66;03m# Now we need to reshape the logits and mask to [B*S,N,...] is num_starts > 1 without dynamic embeddings\u001B[39;00m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# note that rearranging order is important here\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_starts \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_dyn_emb_multi_start:\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\rl4co\\models\\nn\\attention.py:285\u001B[0m, in \u001B[0;36mPointerAttention.forward\u001B[1;34m(self, query, key, value, logit_key, attn_mask)\u001B[0m\n\u001B[0;32m    280\u001B[0m logits \u001B[38;5;241m=\u001B[39m (torch\u001B[38;5;241m.\u001B[39mbmm(glimpse, logit_key\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)))\u001B[38;5;241m.\u001B[39msqueeze(\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[0;32m    282\u001B[0m ) \u001B[38;5;241m/\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(glimpse\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_nan:\n\u001B[1;32m--> 285\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misnan(logits)\u001B[38;5;241m.\u001B[39many(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLogits contain NaNs\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m logits\n",
      "\u001B[1;31mAssertionError\u001B[0m: Logits contain NaNs"
     ]
    }
   ],
   "execution_count": 766
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T11:40:44.869851Z",
     "start_time": "2024-08-13T11:40:44.866333Z"
    }
   },
   "cell_type": "code",
   "source": "print((1 + 20 // 2) % (21))",
   "id": "17946fc8ce448997",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "execution_count": 652
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T11:52:54.756497Z",
     "start_time": "2024-08-13T11:52:54.752674Z"
    }
   },
   "cell_type": "code",
   "source": "(10 + 22 // 2) % (22 + 1)",
   "id": "31d79b0ed2fed69f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 710
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T09:08:53.910869Z",
     "start_time": "2024-08-13T09:08:53.907785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, RichModelSummary\n",
    "\n",
    "from rl4co.envs import PDPEnv\n",
    "from rl4co.models.zoo import AttentionModel\n",
    "from rl4co.utils.trainer import RL4COTrainer"
   ],
   "id": "a3dbb995fd50119d",
   "outputs": [],
   "execution_count": 439
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T07:35:32.439784Z",
     "start_time": "2024-08-13T07:35:32.432629Z"
    }
   },
   "cell_type": "code",
   "source": "env = PDPEnv(generator_params=dict(num_loc=20))",
   "id": "9f410874588e3e39",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T07:35:32.826425Z",
     "start_time": "2024-08-13T07:35:32.798050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AttentionModel(env,\n",
    "                       baseline='rollout',\n",
    "                       train_data_size=100_000, # really small size for demo\n",
    "                       val_data_size=10_000)"
   ],
   "id": "99b11447ca73e3f2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zm0714\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n",
      "C:\\Users\\zm0714\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:208: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T07:35:33.115981Z",
     "start_time": "2024-08-13T07:35:33.111009Z"
    }
   },
   "cell_type": "code",
   "source": "td_init = env.reset(batch_size=[4])",
   "id": "f973045f40d984bc",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T07:35:34.220474Z",
     "start_time": "2024-08-13T07:35:34.216638Z"
    }
   },
   "cell_type": "code",
   "source": "td_init",
   "id": "bbc9bd8b7dab99cc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action_mask: Tensor(shape=torch.Size([4, 21]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        available: Tensor(shape=torch.Size([4, 21]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        current_node: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        depot: Tensor(shape=torch.Size([4, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        i: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
       "        locs: Tensor(shape=torch.Size([4, 21, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([4, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        to_deliver: Tensor(shape=torch.Size([4, 21]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([4]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T07:38:43.615819Z",
     "start_time": "2024-08-13T07:35:37.595531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "td_init = env.reset(batch_size=[4]).to(device)\n",
    "policy = model.policy.to(device)\n",
    "out = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\", return_actions=True)\n",
    "\n",
    "# Plotting\n",
    "print(f\"Tour lengths: {[f'{-r.item():.2f}' for r in out['reward']]}\")\n",
    "for td, actions in zip(td_init, out['actions'].cpu()):\n",
    "    env.render(td, actions)"
   ],
   "id": "68ab9c4364be1fbb",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[97], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m td_init \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset(batch_size\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m4\u001B[39m])\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      3\u001B[0m policy \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m----> 4\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtd_init\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mphase\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgreedy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_actions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Plotting\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTour lengths: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m-\u001B[39mr\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mr\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mout[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreward\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\rl4co\\models\\common\\constructive\\base.py:230\u001B[0m, in \u001B[0;36mConstructivePolicy.forward\u001B[1;34m(self, td, env, phase, calc_reward, return_actions, return_entropy, return_hidden, return_init_embeds, return_sum_log_likelihood, actions, max_steps, **decoding_kwargs)\u001B[0m\n\u001B[0;32m    228\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m td[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdone\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mall():\n\u001B[1;32m--> 230\u001B[0m     logits, mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_starts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m     td \u001B[38;5;241m=\u001B[39m decode_strategy\u001B[38;5;241m.\u001B[39mstep(\n\u001B[0;32m    232\u001B[0m         logits,\n\u001B[0;32m    233\u001B[0m         mask,\n\u001B[0;32m    234\u001B[0m         td,\n\u001B[0;32m    235\u001B[0m         action\u001B[38;5;241m=\u001B[39mactions[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, step] \u001B[38;5;28;01mif\u001B[39;00m actions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    236\u001B[0m     )\n\u001B[0;32m    237\u001B[0m     td \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(td)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\rl4co\\models\\zoo\\am\\decoder.py:186\u001B[0m, in \u001B[0;36mAttentionModelDecoder.forward\u001B[1;34m(self, td, cached, num_starts)\u001B[0m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m num_starts \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    184\u001B[0m     td \u001B[38;5;241m=\u001B[39m unbatchify(td, num_starts)\n\u001B[1;32m--> 186\u001B[0m glimpse_q \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compute_q\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcached\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    187\u001B[0m glimpse_k, glimpse_v, logit_k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compute_kvl(cached, td)\n\u001B[0;32m    189\u001B[0m \u001B[38;5;66;03m# Compute logits\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\rl4co\\models\\zoo\\am\\decoder.py:140\u001B[0m, in \u001B[0;36mAttentionModelDecoder._compute_q\u001B[1;34m(self, cached, td)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m td\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(graph_context_cache, Tensor):\n\u001B[0;32m    138\u001B[0m     graph_context_cache \u001B[38;5;241m=\u001B[39m graph_context_cache\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 140\u001B[0m step_context \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontext_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode_embeds_cache\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    141\u001B[0m glimpse_q \u001B[38;5;241m=\u001B[39m step_context \u001B[38;5;241m+\u001B[39m graph_context_cache\n\u001B[0;32m    142\u001B[0m \u001B[38;5;66;03m# add seq_len dim if not present\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\rl4co\\models\\nn\\env_embeddings\\context.py:248\u001B[0m, in \u001B[0;36mPDPContext.forward\u001B[1;34m(self, embeddings, td)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, embeddings, td):\n\u001B[1;32m--> 248\u001B[0m     cur_node_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cur_node_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtd\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproject_context(cur_node_embedding)\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\rl4co\\models\\nn\\env_embeddings\\context.py:59\u001B[0m, in \u001B[0;36mEnvContext._cur_node_embedding\u001B[1;34m(self, embeddings, td)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_cur_node_embedding\u001B[39m(\u001B[38;5;28mself\u001B[39m, embeddings, td):\n\u001B[0;32m     58\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get embedding of current node\"\"\"\u001B[39;00m\n\u001B[1;32m---> 59\u001B[0m     cur_node_embedding \u001B[38;5;241m=\u001B[39m \u001B[43mgather_by_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtd\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcurrent_node\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cur_node_embedding\n",
      "File \u001B[1;32m~\\Documents\\Projekte\\rl4co-slap\\rl4co\\utils\\ops.py:77\u001B[0m, in \u001B[0;36mgather_by_index\u001B[1;34m(src, idx, dim, squeeze)\u001B[0m\n\u001B[0;32m     75\u001B[0m idx \u001B[38;5;241m=\u001B[39m idx\u001B[38;5;241m.\u001B[39mview(idx\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1\u001B[39m,) \u001B[38;5;241m*\u001B[39m (src\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m-\u001B[39m idx\u001B[38;5;241m.\u001B[39mdim()))\u001B[38;5;241m.\u001B[39mexpand(expanded_shape)\n\u001B[0;32m     76\u001B[0m squeeze \u001B[38;5;241m=\u001B[39m idx\u001B[38;5;241m.\u001B[39msize(dim) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m squeeze\n\u001B[1;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m src\u001B[38;5;241m.\u001B[39mgather(dim, idx)\u001B[38;5;241m.\u001B[39msqueeze(dim) \u001B[38;5;28;01mif\u001B[39;00m \u001B[43msqueeze\u001B[49m \u001B[38;5;28;01melse\u001B[39;00m src\u001B[38;5;241m.\u001B[39mgather(dim, idx)\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1103\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1065\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2024.1.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1201\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1198\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1200\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1201\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2024.1.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1216\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1213\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1215\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1216\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1218\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1220\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d12f32bb69d0e5af"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
